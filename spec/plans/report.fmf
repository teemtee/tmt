summary: Report test results
story:
    As a tester I want to have a nice overview of results once
    the testing if finished.
description:
    Report test results according to user preferences.

/display:
    summary: Show results in the terminal window
    story:
        As a tester I want to see test results in the plain text
        form in my shell session.
    description:
        Test results will be displayed as part of the command line
        tool output directly in the terminal. Allows to select the
        desired level of verbosity
    example: |
        tmt run -l report        # overall summary only
        tmt run -l report -v     # individual test results
        tmt run -l report -vv    # show full paths to logs
        tmt run -l report -vvv   # provide complete test output
    link:
      - implemented-by: /tmt/steps/report/display.py

/html:
    summary: Generate a web page with test results
    story:
        As a tester I want to review results in a nicely arranged
        web page with links to detailed test output.
    description:
        Create a local ``html`` file with test results arranged in
        a table. Optionally open the page in the default browser.
    example: |
        # Enable html report from the command line
        tmt run --all report --how html
        tmt run --all report --how html --open
        tmt run -l report -h html -o

        # Use html as the default report for given plan
        report:
            how: html
            open: true
    link:
      - implemented-by: /tmt/steps/report/html.py

/junit:
    summary: Generate a JUnit report file
    story:
        As a tester I want to review results in a JUnit xml file.
    description:
        Create a JUnit file ``junit.xml`` with test results.
    example: |
        # Enable junit report from the command line
        tmt run --all report --how junit
        tmt run --all report --how junit --file test.xml

        # Use junit as the default report for given plan
        report:
            how: junit
            file: test.xml
    link:
        - implemented-by: /tmt/steps/report/junit.py

/polarion:
    summary: Generate a xUnit file and export it into Polarion
    story:
        As a tester I want to review tests in Polarion
        and have all results linked to existing test cases there.
    description: |
        Create a xUnit file ``xunit.xml`` with test results
        and Polarion properties so the xUnit can then be
        exported into Polarion.

        .. note::

            Your Polarion project might need a custom value format
            for the ``arch``, ``planned-in`` and other fields. The
            format of these fields might differ across Polarion
            projects, for example, ``x8664`` can be used instead
            of ``x86_64`` for the architecture.

    example:
      - |
        # Enable polarion report from the command line
        tmt run --all report --how polarion --project-id tmt
        tmt run --all report --how polarion --project-id tmt --no-upload --file test.xml

      - |
        # Use polarion as the default report for given plan
        report:
            how: polarion
            file: test.xml
            project-id: tmt
            title: tests_that_pass
            planned-in: RHEL-9.1.0
            pool-team: sst_tmt
    link:
        - implemented-by: /tmt/steps/report/polarion.py

/reportportal:
    summary: Report test results to a ReportPortal instance
    story:
        As a tester I want to review results in a nicely arranged
        web page, filter them via context attributes and get links
        to detailed test output and other test information.
    description:
        Provide test results and fmf data per each plan,
        and send it to a Report Portal instance via its API
        with token, url and project name given.
    example:
      - |
        # Optionally set environment variables according to TMT_PLUGIN_REPORT_REPORTPORTAL_${OPTION}
        export TMT_PLUGIN_REPORT_REPORTPORTAL_URL=${url-to-RP-instance}
        export TMT_PLUGIN_REPORT_REPORTPORTAL_TOKEN=${token-from-RP-profile}
      - |
        # Enable ReportPortal report from the command line depending on the use case:

        ## Simple upload with all project, url endpoint and user token passed in command line
        tmt run --all report --how reportportal --project=baseosqe --url="https://reportportal.xxx.com" --token="abc...789"

        ## Simple upload with url and token exported in environment variable
        tmt run --all report --how reportportal --project=baseosqe

        ## Upload with project name in fmf data, filtering out parameters (environemnt variables) that tend to be unique and break the history aggregation
        tmt run --all report --how reportportal --exclude-variables="^(TMT|PACKIT|TESTING_FARM).*"

        ## Upload all plans as suites into one ReportPortal launch
        tmt run --all report --how reportportal --suite-per-plan --launch=Errata --launch-description="..."

        ## Rerun the launch with suite structure for the test results to be uploaded into the latest launch with the same name as a new 'Retry' tab (mapping based on unique paths)
        tmt run --all report --how reportportal --suite-per-plan --launch=Errata --launch-rerun

        ## Rerun the tmt run and append the new result logs under the previous one uploaded in ReportPortal (precise mapping)
        tmt run --id run-012 --all report --how reportportal --again

        ## Additional upload of new suites into given launch with suite structure
        tmt run --all report --how reportportal --suite-per-plan --upload-to-launch=4321

        ## Additional upload of new tests into given launch with non-suite structure
        tmt run --all report --how reportportal --launch-per-plan --upload-to-launch=1234

        ## Additional upload of new tests into given suite
        tmt run --all report --how reportportal --upload-to-suite=123456

        ## Upload Idle tests, then execute it and add result logs into prepared empty tests
        tmt run discover report --how reportportal --defect-type=Idle
        tmt run --last --all report --how reportportal --again
      - |
        # Use ReportPortal as the default report for given plan
        report:
            how: reportportal
            project: baseosqe

        # Report context attributes for given plan
        context:
            ...
      - |
        # Report description, contact, id and environment variables for given test
        summary: ...
        contact: ...
        id: ...
        environment:
            ...
    link:
        - implemented-by: /tmt/steps/report/reportportal.py

/file:
    description: |

        Save the report into a ``report.yaml`` file with the
        following format:

        .. code-block:: yaml

            result: OVERALL_RESULT
            plans:
                /plan/one:
                    result: PLAN_RESULT
                    tests:
                        /test/one:
                            result: TEST_RESULT
                            log:
                              - LOG_PATH

                        /test/two:
                            result: TEST_RESULT
                            log:
                                - LOG_PATH
                                - LOG_PATH
                                - LOG_PATH
                /plan/two:
                    result: PLAN_RESULT
                        /test/one:
                            result: TEST_RESULT
                            log:
                              - LOG_PATH

        Where ``OVERALL_RESULT`` is the overall result of all plan
        results. It is counted the same way as ``PLAN_RESULT``.

        Where ``TEST_RESULT`` is the same as in `execute`_ step
        definition:

            * info - test finished and produced only information
              message
            * passed - test finished and passed
            * failed - test finished and failed
            * error - a problem encountered during test execution

        Note the priority  of test results is as written above,
        with ``info`` having the lowest priority and ``error`` has
        the highest. This is important for ``PLAN_RESULT``.

        Where ``PLAN_RESULT`` is the overall result or all test
        results for the plan run. It has the same values as
        ``TEST_RESULT``. Plan result is counted according to the
        priority of the test outcome values. For example:

            * if the test results are info, passed, passed - the
              plan result will be passed
            * if the test results are info, passed, failed - the
              plan result will be failed
            * if the test results are failed, error, passed - the
              plan result will be error

        Where ``LOG_PATH`` is the test log output path, relative
        to the execute step plan run directory. The ``log`` key
        will be a list of such paths, even if there is just a single
        log.
